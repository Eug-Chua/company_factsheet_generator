company_name: "capitaland"  # Options: capitaland, grab, sea, sq, weride

# file Paths
data_folder: "data"
markdown_folder: "markdown_files"
outputs_folder: "outputs"
logs_folder: "logs"
question_set_path: "question_set.md"

pdf_files:
  capitaland: "Capitaland 2024.pdf"
  grab: "Grab 2024.pdf"
  sea: "SEA 2024.pdf"
  sq: "SQ 2024.pdf"
  weride: "WeRide 2024.pdf"
  tsla: "Tesla 2024.pdf"

embedding_model: "all-MiniLM-L6-v2"

# LLM Configuration
# Supported providers: "openai" (main), "ollama" (backup/local)
llm_provider: "openai"
# openai_model: "gpt-4o-mini"
openai_model: "gpt-4.1-nano"
ollama_model: "qwen3:14b"
ollama_url: "http://localhost:11434"

docling:
  export_format: "markdown"

chunking:
  min_chunk_size: 200  # Increased to prevent empty header chunks
  max_chunk_size: 2000

semantic_chunking:
  similarity_threshold: 0.6
  max_merged_size: 4000

# Multi-HyDE Configuration
multi_hyde:
  enabled: true  # Single flag - enables Multi-HyDE for qualitative questions (Q1-9)
  num_variants: 5
  k_per_hypothetical: 10
  cross_encoder_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"