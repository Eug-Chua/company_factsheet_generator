company_name: "capitaland"  # Options: capitaland, dbs, grab, sq

# file Paths
data_folder: "data"
markdown_folder: "markdown_files"
outputs_folder: "outputs"
logs_folder: "logs"
question_set_path: "question_set.md"

pdf_files:
  capitaland: "Capitaland 2024.pdf"
  grab: "Grab 2024.pdf"
  sq: "SQ 2024.pdf"
  sea: "SEA 2024.pdf"
  weride: "WeRide 2024.pdf"
  tsla: "Tesla 2024.pdf"

embedding_model: "all-MiniLM-L6-v2"

llm_provider: "openai"
claude_model: "claude-3-haiku-20240307"
openai_model: "gpt-4o-mini"
ollama_model: "qwen3:14b"
ollama_url: "http://localhost:11434"

docling:
  export_format: "markdown"

chunking:
  min_chunk_size: 200  # Increased to prevent empty header chunks
  max_chunk_size: 2000

semantic_chunking:
  similarity_threshold: 0.6
  max_merged_size: 4000

# Multi-HyDE Configuration
multi_hyde:
  enabled: true  # Set to true to enable Multi-HyDE retrieval
  num_variants: 5  # Number of query variants to generate (paper uses 3-5)
  k_per_hypothetical: 10  # Top-k chunks to retrieve per hypothetical doc (paper uses 10)
  use_cross_encoder: true  # Set to true to use cross-encoder for final reranking (slower but more accurate)
  cross_encoder_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"  # Cross-encoder model for reranking 
